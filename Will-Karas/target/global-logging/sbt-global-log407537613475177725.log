[0m[[0m[0mdebug[0m] [0m[0m> Exec(run, Some(022fc91c-c583-4bec-8649-e317c0b11d53), Some(CommandSource(console0)))[0m[0J
[0m[[0m[0mdebug[0m] [0m[0mEvaluating tasks: Compile / run[0m[0J
[0m[[0m[0mdebug[0m] [0m[0mRunning task... Cancel: bloop.integrations.sbt.Offloader$$anon$1@4be12f6c, check cycles: false, forcegc: true[0m[0J
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: initialized: JsonRpcNotificationMessage(2.0, initialized, {})[0m[0J
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: textDocument/didOpen: JsonRpcNotificationMessage(2.0, textDocument/didOpen, {"textDocument":{"uri":"file:///c%3A/Users/gmixo/Desktop/Will-Karas/src/main/scala/clustering/cure/CureAlgorithm.scala","languageId":"scala","version":3,"text":"package clustering.cure\r\n\r\nimport scala.math.sqrt\r\nimport clustering.structures.{Cluster, KDNode, KDPoint, KDTree, MinHeap}\r\nimport org.apache.spark.SparkContext\r\nimport org.apache.spark.broadcast.Broadcast\r\nimport org.apache.spark.rdd.RDD\r\n\r\nobject CureAlgorithm {\r\n\r\n  def start(cureArgs: CureArgs, sparkContext: SparkContext): RDD[(Array[Double], Int)] = {\r\n\r\n    val distFile = sparkContext.textFile(cureArgs.inputFile)\r\n      .map(_\r\n        .split(\",\")\r\n        .map(_.toDouble)\r\n      )\r\n\r\n    val sample = distFile\r\n      .sample(withReplacement = false, fraction = cureArgs.samplingRatio)\r\n      .repartition(cureArgs.partitions)\r\n    println(s\"The total size is ${distFile.count()} and sampled count is ${sample.count()}\")\r\n\r\n    val points = sample.map(a => {\r\n      val p = KDPoint(a)\r\n      p.cluster = Cluster(Array(p), Array(p), null, p)\r\n      p\r\n    }).cache()\r\n\r\n    val broadcastVariables = (sparkContext.broadcast(cureArgs.numClusters),\r\n      sparkContext.broadcast(cureArgs.numRepresentatives),\r\n      sparkContext.broadcast(cureArgs.shrinkingFactor),\r\n      sparkContext.broadcast(cureArgs.removeOutliers))\r\n\r\n    val clusters = points.mapPartitions(partition => cluster(partition, broadcastVariables))\r\n      .collect()\r\n    println(s\"Partitioned Execution finished Successfully. Collected all ${clusters.length} clusters at driver.\")\r\n    println(clusters)\r\n\r\n\r\n    def EuclideanDistance(x: Array[Double], y: Array[Double]) = {\r\n      scala.math.sqrt((x zip y).map { case (x,y) => scala.math.pow(y - x, 2.0) }.sum)\r\n    }\r\n\r\n    //centers= clusters.EuclideanDistance\r\n\r\n    val reducedPoints = clusters.flatMap(_.representatives).toList\r\n    val kdTree = createKDTree(reducedPoints)\r\n    val cHeap = createHeapFromClusters(clusters.toList, kdTree)\r\n\r\n    var clustersShortOfMReps =\r\n      if (cureArgs.removeOutliers)\r\n        clusters.count(_.representatives.length < cureArgs.numRepresentatives)\r\n      else\r\n        0\r\n\r\n    // trim all clusters having less than the desired number of representatives\r\n    while (cHeap.heapSize - clustersShortOfMReps > cureArgs.numClusters) {\r\n      val c1 = cHeap.takeHead()\r\n      val nearest = c1.nearest\r\n      val c2 = merge(c1, nearest, cureArgs.numRepresentatives, cureArgs.shrinkingFactor)\r\n\r\n      if (cureArgs.removeOutliers) {\r\n        val a = nearest.representatives.length < cureArgs.numRepresentatives\r\n        val b = c1.representatives.length < cureArgs.numRepresentatives\r\n        val c = c2.representatives.length < cureArgs.numRepresentatives\r\n\r\n        if (a && b && c) clustersShortOfMReps = clustersShortOfMReps - 1\r\n        else if (a && b) clustersShortOfMReps = clustersShortOfMReps - 2\r\n        else if (a || b) clustersShortOfMReps = clustersShortOfMReps - 1\r\n      }\r\n\r\n      c1.representatives.foreach(kdTree.delete)\r\n      nearest.representatives.foreach(kdTree.delete)\r\n\r\n      val (newNearestCluster, nearestDistance) = getNearestCluster(c2, kdTree)\r\n      c2.nearest = newNearestCluster\r\n      c2.squaredDistance = nearestDistance\r\n\r\n      c2.representatives.foreach(kdTree.insert)\r\n      removeClustersFromHeapUsingReps(kdTree, cHeap, c1, nearest)\r\n      cHeap.insert(c2)\r\n      println(s\"Processing and merging clusters. Heap size is ${cHeap.heapSize}\")\r\n    }\r\n    println(s\"Merged clusters at driver.\\n\" +\r\n      s\"  Total clusters ${cHeap.heapSize}\\n\" +\r\n      s\"  Removed $clustersShortOfMReps clusters without ${cureArgs.numRepresentatives} representatives\")\r\n\r\n    val finalClusters = cHeap.getDataArray\r\n      .slice(0, cHeap.heapSize)\r\n      .filter(_.representatives.length >= cureArgs.numRepresentatives)\r\n    finalClusters.zipWithIndex\r\n      .foreach { case (x, i) => x.id = i }\r\n\r\n    println(\"Final Representatives\")\r\n    finalClusters\r\n      .foreach(c =>\r\n        c.representatives\r\n          .foreach(r => println(s\"$r , ${c.id}\"))\r\n      )\r\n\r\n    val broadcastTree = sparkContext.broadcast(kdTree)\r\n    distFile.mapPartitions(partition => {\r\n      partition.map(p => {\r\n        (p, broadcastTree.value\r\n          .closestPointOfOtherCluster(KDPoint(p))\r\n          .cluster\r\n          .id)\r\n      })\r\n    })\r\n  }\r\n\r\n  private def cluster(partition: Iterator[KDPoint],\r\n                      broadcasts: (Broadcast[Int],\r\n                        Broadcast[Int],\r\n                        Broadcast[Double],\r\n                        Broadcast[Boolean])): Iterator[Cluster] = {\r\n\r\n    val (numClusters, numRepresentatives, shrinkingFactor, removeOutliers) = broadcasts\r\n\r\n    val partitionList = partition.toList\r\n\r\n    if (partitionList.length <= numClusters.value)\r\n      return partitionList\r\n        .map(p => Cluster(Array(p), Array(p), null, p))\r\n        .toIterator\r\n\r\n    val kdTree = createKDTree(partitionList)\r\n    val cHeap = createHeap(partitionList, kdTree)\r\n\r\n    if (removeOutliers.value) {\r\n      computeClustersAtPartitions(numClusters.value * 2, numRepresentatives.value, shrinkingFactor.value, kdTree, cHeap)\r\n      for (i <- 0 until cHeap.heapSize)\r\n        if (cHeap.getDataArray(i).representatives.length < numRepresentatives.value)\r\n          cHeap.remove(i)\r\n    }\r\n    computeClustersAtPartitions(numClusters.value, numRepresentatives.value, shrinkingFactor.value, kdTree, cHeap)\r\n\r\n    cHeap.getDataArray\r\n      .slice(0, cHeap.heapSize)\r\n      .map(c => {\r\n        c.points.foreach(_.cluster = null)\r\n        val newCluster = Cluster(findMFarthestPoints(c.points, c.mean, numRepresentatives.value),\r\n          c.representatives,\r\n          null,\r\n          c.mean,\r\n          c.squaredDistance)\r\n        newCluster.representatives.foreach(_.cluster = newCluster)\r\n        newCluster\r\n      }).toIterator\r\n  }\r\n\r\n  private def createKDTree(data: List[KDPoint]): KDTree = {\r\n    val kdTree = KDTree(KDNode(data.head, null, null), data.head.dimensions.length)\r\n    for (i <- 1 until data.length)\r\n      kdTree.insert(data(i))\r\n    kdTree\r\n  }\r\n\r\n  private def createHeap(data: List[KDPoint], kdTree: KDTree) = {\r\n    val cHeap = MinHeap(data.length)\r\n    data.map(p => {\r\n      val closest = kdTree.closestPointOfOtherCluster(p)\r\n      p.cluster.nearest = closest.cluster\r\n      p.cluster.squaredDistance = p.squaredDistance(closest)\r\n      cHeap.insert(p.cluster)\r\n      p.cluster\r\n    })\r\n    cHeap\r\n  }\r\n\r\n  private def createHeapFromClusters(data: List[Cluster], kdTree: KDTree): MinHeap = {\r\n    val cHeap = MinHeap(data.length)\r\n    data.foreach(p => {\r\n      val (closest, distance) = getNearestCluster(p, kdTree)\r\n      p.nearest = closest\r\n      p.squaredDistance = distance\r\n      cHeap.insert(p)\r\n    })\r\n    cHeap\r\n  }\r\n\r\n  private def computeClustersAtPartitions(numClusters: Int,\r\n                                          numRepresentatives: Int,\r\n                                          sf: Double,\r\n                                          kdTree: KDTree,\r\n                                          cHeap: MinHeap): Unit = {\r\n    while (cHeap.heapSize > numClusters) {\r\n      val c1 = cHeap.takeHead()\r\n      val nearest = c1.nearest\r\n      val c2 = merge(c1, nearest, numRepresentatives, sf)\r\n\r\n      c1.representatives.foreach(kdTree.delete)\r\n      nearest.representatives.foreach(kdTree.delete)\r\n\r\n      val (newNearestCluster, nearestDistance) = getNearestCluster(c2, kdTree)\r\n      c2.nearest = newNearestCluster\r\n      c2.squaredDistance = nearestDistance\r\n      c2.representatives.foreach(kdTree.insert)\r\n\r\n      removeClustersFromHeapUsingReps(kdTree, cHeap, c1, nearest)\r\n\r\n      cHeap.insert(c2)\r\n    }\r\n  }\r\n\r\n  private def removeClustersFromHeapUsingReps(kdTree: KDTree, cHeap: MinHeap, cluster: Cluster, nearest: Cluster): Unit = {\r\n    val heapSize = cHeap.heapSize\r\n    var i = 0\r\n    while (i < heapSize) {\r\n      var continue = true\r\n      val currCluster = cHeap.getDataArray(i)\r\n      val currNearest = currCluster.nearest\r\n      if (currCluster == nearest) {\r\n        cHeap.remove(i)\r\n        continue = false\r\n      }\r\n      if (currNearest == nearest || currNearest == cluster) {\r\n        val (newCluster, newDistance) = getNearestCluster(currCluster, kdTree)\r\n        currCluster.nearest = newCluster\r\n        currCluster.squaredDistance = newDistance\r\n        cHeap.heapify(i)\r\n        continue = false\r\n      }\r\n      if (continue) i += 1\r\n    }\r\n  }\r\n\r\n  private def getNearestCluster(cluster: Cluster, kdTree: KDTree): (Cluster, Double) = {\r\n    val (nearestRep, nearestDistance) = cluster\r\n      .representatives\r\n      .foldLeft(null: KDPoint, Double.MaxValue) {\r\n        case ((currNearestRep, currNearestDistance), rep) =>\r\n          val nearestRep = kdTree.closestPointOfOtherCluster(rep)\r\n          val nearestDistance = rep.squaredDistance(nearestRep)\r\n          if (nearestDistance < currNearestDistance)\r\n            (nearestRep, nearestDistance)\r\n          else\r\n            (currNearestRep, currNearestDistance)\r\n      }\r\n    (nearestRep.cluster, nearestDistance)\r\n  }\r\n\r\n  def copyPointsArray(oldArray: Array[KDPoint]): Array[KDPoint] = {\r\n    oldArray\r\n      .clone()\r\n      .map(p => {\r\n        if (p == null)\r\n          return null\r\n        KDPoint(p.dimensions.clone())\r\n      })\r\n  }\r\n\r\n  private def merge(cluster: Cluster, nearest: Cluster, repCount: Int, sf: Double): Cluster = {\r\n    val mergedPoints = cluster.points ++ nearest.points\r\n    val mean = meanOfPoints(mergedPoints)\r\n    var representatives = mergedPoints\r\n    if (mergedPoints.length > repCount)\r\n      representatives = findMFarthestPoints(mergedPoints, mean, repCount)\r\n    representatives = shrinkRepresentativeArray(sf, representatives, mean)\r\n\r\n    val mergedCl = Cluster(mergedPoints, representatives, null, mean)\r\n\r\n    mergedCl.representatives.foreach(_.cluster = mergedCl)\r\n    mergedCl.points.foreach(_.cluster = mergedCl)\r\n    mergedCl.mean.cluster = mergedCl\r\n\r\n    mergedCl\r\n  }\r\n\r\n  private def findMFarthestPoints(points: Array[KDPoint], mean: KDPoint, m: Int): Array[KDPoint] = {\r\n    val tmpArray = new Array[KDPoint](m)\r\n    for (i <- 0 until m) {\r\n      var maxDist = 0.0d\r\n      var minDist = 0.0d\r\n      var maxPoint: KDPoint = null\r\n\r\n      points.foreach(p => {\r\n        if (!tmpArray.contains(p)) {\r\n          if (i == 0) minDist = p.squaredDistance(mean)\r\n          else {\r\n            minDist = tmpArray.foldLeft(Double.MaxValue) { (maxd, r) => {\r\n              if (r == null) maxd\r\n              else {\r\n                val dist = p.squaredDistance(r)\r\n                if (dist < maxd) dist\r\n                else maxd\r\n              }\r\n            }\r\n            }\r\n          }\r\n          if (minDist >= maxDist) {\r\n            maxDist = minDist\r\n            maxPoint = p\r\n          }\r\n        }\r\n      })\r\n      tmpArray(i) = maxPoint\r\n    }\r\n    tmpArray.filter(_ != null)\r\n  }\r\n\r\n  private def shrinkRepresentativeArray(sf: Double, repArray: Array[KDPoint], mean: KDPoint): Array[KDPoint] = {\r\n    val tmpArray = copyPointsArray(repArray)\r\n    tmpArray.foreach(rep => {\r\n      val repDim = rep.dimensions\r\n      repDim.indices\r\n        .foreach(i => repDim(i) += (mean.dimensions(i) - repDim(i)) * sf)\r\n    })\r\n    tmpArray\r\n  }\r\n\r\n  def meanOfPoints(points: Array[KDPoint]): KDPoint = {\r\n    KDPoint(points\r\n      .filter(_ != null)\r\n      .map(_.dimensions)\r\n      .transpose\r\n      .map(x => {\r\n        x.sum / x.length\r\n      }))\r\n  }\r\n  \r\n}\r\n"}})[0m[0J
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: textDocument/didOpen: JsonRpcNotificationMessage(2.0, textDocument/didOpen, {"textDocument":{"uri":"file:///c%3A/Users/gmixo/Desktop/Will-Karas/src/main/scala/bi-kmeans.scala","languageId":"scala","version":1,"text":"import org.apache.spark.mllib.clustering.BisectingKMeans\nimport org.apache.spark.mllib.linalg.{Vectors, Vector}\nimport org.apache.spark.ml.evaluation.ClusteringEvaluator\n\nimport org.apache.log4j._\nimport org.apache.spark.{SparkConf, SparkContext}\n\nimport java.io.PrintWriter\nimport java.util.Date\n\nobject bikmeans {\n\n  def main(args: Array[String]): Unit = {\n\n    Logger.getLogger(\"org.apache.spark.SparkContext\").setLevel(Level.WARN)\n\n    val sparkConf = new SparkConf()\n      .setMaster(\"local[*]\")\n      .setAppName(\"BI-KMEANS\")\n\n    val sc = new SparkContext(sparkConf)\n\nval currentDir = System.getProperty(\"user.dir\")\nval data = \"file:///\" + currentDir + \"\\\\data1.txt\"\nval outputDir = \"file:///\" + currentDir + \"\\\\output_bikmeans\"\nval startTime = System.currentTimeMillis()\nval parsedData = sc.textFile(data).map(s => Vectors.dense(s.split(',').map(_.toDouble))).cache()\n\n// Cluster the data into two classes using KMeans\nval numClusters = 5\n\nval numIterations = 40\nval k= numClusters\n\nval bkm = new BisectingKMeans().setK(numClusters).setMaxIterations(numIterations)\nval clusters = bkm.run(parsedData)\nprintln(clusters)\nval WSSSE = clusters.computeCost(parsedData)\nval endTime = System.currentTimeMillis()\n\n\nprintln(s\"Compute Cost: ${WSSSE}\")\nval text = s\"Total time taken to assign clusters is : ${((endTime - startTime) * 1.0) / 1000} seconds\"\nprintln(text)\nclusters.clusterCenters.zipWithIndex.foreach { case (center, idx) =>\nprintln(s\"Cluster Center ${idx}: ${center}\")}\n\n\n\n//Returns the Euclidian distance between vectors v1,v2\n\tdef vecDist( v1:Vector, v2:Vector ) : Double = {\n\t  return Math.sqrt(Vectors.sqdist(v1,v2))\n\t}\n\n\tval clusteredData = parsedData.map(v => (clusters.predict(v),v)).cache() //Key: index of the appropriate cluster, Value: normalized point\n\t\t\t\t\t\n\tval indexedCenters = sc.parallelize(clusters.clusterCenters.map(cntr => (clusters.predict(cntr),cntr))).cache() //Key: index, Value: cluster center\n\n\t//Find the mean Silhouette score and the outliers for every cluster\n\tfor (c <- 0 to k-1)\n\t{\n\t\t//The points from current cluster (c)\n\t\tval currData = clusteredData.filter(r => r._1 == c)\n\t\t\t.map(r => r._2).cache() //They are all from the same cluster so we throw the cluster index\n\t\tval indexedCurrData = currData.zipWithIndex().cache() //We index them in order to give an id to every point\n\t\tval numPoints = currData.count() //Number of points in this cluster\n\n\t\t//Cartesian product in order to compute each point's distance from all other points in the same cluster\n\t\tval intraClusterProduct = indexedCurrData.cartesian(indexedCurrData)\n\t\t\t.map{case (x,y) => (x._2,vecDist(x._1,y._1))} //Key: the first point's id (from when it was indexed), Value: its distance from the second point. \n\t\t//In the cartesian product we also have points paired with themselves, but they don't affect our computations because their distance is 0 (doesn't affect the sum) and we divide by numPoints-1 (so that it doesn't affect the average)\n\t\t//ai = object i's average distance from the other objects in its cluster\n\t\tval ai = intraClusterProduct.reduceByKey((a,b) => a + b ) //Add the distances for elements with the same id (same point)\n\t\t\t.map(x =>(x._1, x._2/(numPoints-1))) //Key: point id, Value: average distance from its cluster\n    \t\t\t\n\t\t//All centers except the current one in order to find the closest to every point\n\t\tval otherCenters = indexedCenters.filter(x => x._1!=c )\n\t\tval nearestCenters = indexedCurrData.cartesian(otherCenters) //Every point from this cluster with every other center\n\t\t\t.map(r => (r._1._2,(r._2._1,r._1._1,vecDist(r._1._1,r._2._2)))) //Key: point id, Value: tuple (center index, point vector) \n\t\t\t.reduceByKey((a,b) => if (a._3 < b._3) a else b) //Min distance for every id \n\t\t\t.map(r => (r._2._1,(r._1, r._2._2))) //Key: index to center with min distance, Value: tuple(point id, point vector)\n\t\t//bi = object i's minimum average distance from objects of another cluster\n\t\tval bi = nearestCenters.join(clusteredData) //Join each point of this cluster with every point in the cluster with the nearest center (A workaround for computing bi granted that the data follow normal distribution)\n\t\t\t.map(r => (r._2._1._1,(vecDist(r._2._1._2,r._2._2),1))) //Key: first point's id, Value: tuple(distance of the two points,1)\n\t\t\t.reduceByKey((a,b) =>( a._1 + b._1, a._2 + b._2)) //Add the distances for every id (point) and count them \n\t\t\t.map(r => (r._1, r._2._1/r._2._2)) //Key: point id, Value: average distance from nearest cluster\n\n\t\tdef maxD(d1 : Double, d2: Double): Double = {\n\t\t\tif (d1>d2) return d1 else return d2\n\t\t}\n\t\t\n\t\tval s = ai.join(bi) //Join on point id\n\t\t\t.map(r => ((r._2._2 - r._2._1) / maxD(r._2._2,r._2._1), 1) ) //Key: Silhouette score for every point, Value: 1\n\t\t\t.reduce((a,b) => (a._1 + b._1, a._2 + b._2)) //Sum of silhouette scores, number of points\n\t\tval meanS = s._1 / s._2 //Average silhouette score for this cluster\t\t\n\n    println(\"Cluster \"+ (c+1)) \n\t\tprintln(\"Mean Silhouette Score: \"+ meanS)\t\n\t\tprintln()\t\n  }\n\n// Save and load model\nclusters.save(sc, outputDir)\n\nsc.stop() \n}\n}\n"}})[0m[0J
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: textDocument/didOpen: JsonRpcNotificationMessage(2.0, textDocument/didOpen, {"textDocument":{"uri":"file:///c%3A/Users/gmixo/Desktop/Will-Karas/src/main/scala/Cure.scala","languageId":"scala","version":1,"text":"import clustering.cure.{CureAlgorithm, CureArgs}\r\nimport org.apache.hadoop.conf.Configuration\r\nimport org.apache.hadoop.fs.{FileSystem, Path}\r\nimport org.apache.log4j._\r\nimport org.apache.spark.{SparkConf, SparkContext}\r\n\r\nimport java.io.PrintWriter\r\nimport java.util.Date\r\n\r\nobject Cure {\r\n\r\n  def main(args: Array[String]): Unit = {\r\n    Logger.getLogger(\"org.apache.spark.SparkContext\").setLevel(Level.WARN)\r\n\r\n    val sparkConf = new SparkConf()\r\n      .setMaster(\"local[*]\")\r\n      .setAppName(\"Cure\")\r\n\r\n    val sc = new SparkContext(sparkConf)\r\n    \r\n    val startTime = System.currentTimeMillis()\r\n    val currentDir = System.getProperty(\"user.dir\")\r\n    val inputDir = \"file:///\" + currentDir + \"\\\\data1.txt\"\r\n    val outputDir = \"file:///\" + currentDir + \"\\\\output_cure\"\r\n\r\n\r\n    val numClusters = 5\r\n    val numRepresentatives=5\r\n    val shrinkingFactor= 0.3\r\n    val partitions = 1\r\n    val inputFile = inputDir\r\n    val samplingRatio = 0.4\r\n    val removeOutliers= false\r\n\r\n    val cureArgs = CureArgs(numClusters, numRepresentatives, shrinkingFactor, partitions, inputFile, samplingRatio, removeOutliers)\r\n\r\n    \r\n    val result = CureAlgorithm.start(cureArgs, sc).cache()\r\n    println(result)\r\n    val endTime = System.currentTimeMillis()\r\n\r\n    val text = s\"Total time taken to assign clusters is : ${((endTime - startTime) * 1.0) / 1000} seconds\"\r\n    println(text)\r\n\r\n\r\n    val resultFile = outputDir + \"_\" + new Date().getTime.toString\r\n    result.map(x =>\r\n      x._1\r\n        .mkString(\",\")\r\n        .concat(s\",${x._2}\")\r\n    ).saveAsTextFile(resultFile)\r\n\r\n    val conf = new Configuration()\r\n    val fs = FileSystem.get(conf)\r\n    val output = fs.create(new Path(s\"$resultFile/runtime.txt\"))\r\n    val writer = new PrintWriter(output)\r\n    try\r\n      writer.write(text)\r\n    finally\r\n      writer.close()\r\n\r\n    sc.stop()\r\n  }\r\n}\r\n"}})[0m[0J
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: textDocument/didOpen: JsonRpcNotificationMessage(2.0, textDocument/didOpen, {"textDocument":{"uri":"file:///c%3A/Users/gmixo/Desktop/Will-Karas/src/main/scala/Kmeans.scala","languageId":"scala","version":1,"text":"import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}\nimport org.apache.spark.mllib.linalg.{Vectors, Vector}\nimport org.apache.spark.ml.evaluation.ClusteringEvaluator\n\nimport org.apache.log4j._\nimport org.apache.spark.{SparkConf, SparkContext}\nimport java.io.PrintWriter\nimport java.util.Date\n\n\nobject kmeans {\n\n  def main(args: Array[String]): Unit = {\n\n    Logger.getLogger(\"org.apache.spark.SparkContext\").setLevel(Level.WARN)\n\n    val sparkConf = new SparkConf()\n      .setMaster(\"local[*]\")\n      .setAppName(\"KMEANS\")\n\n    val sc = new SparkContext(sparkConf)\n\nval currentDir = System.getProperty(\"user.dir\")\nval data = \"file:///\" + currentDir + \"\\\\data_size1\\\\*\"\nval outputDir = \"file:///\" + currentDir + \"\\\\output_kmeans\"\n\n\nval startTime = System.currentTimeMillis()\nval parsedData = sc.textFile(data).map(s => Vectors.dense(s.split(',').map(_.toDouble))).cache()\n\n// Cluster the data into two classes using KMeans\nval numClusters = 5\n\nval numIterations = 20\nval initializationmode= \"KMeans.K_MEANS_PARALLEL\"\nval clusters = KMeans.train(parsedData, numClusters, numIterations, initializationmode)\nval k = numClusters \n// Evaluate clustering by computing Within Set Sum of Squared Errors\n\n\nval WSSSE = clusters.computeCost(parsedData)\nprintln(s\"Within Set Sum of Squared Errors or Compute Cost = $WSSSE\")\nval endTime = System.currentTimeMillis()\n\nval text = s\"Total time taken to assign clusters is : ${((endTime - startTime) * 1.0) / 1000} seconds\"\nprintln(text)\n\n//Returns the Euclidian distance between vectors v1,v2\n\tdef vecDist( v1:Vector, v2:Vector ) : Double = {\n\t  return Math.sqrt(Vectors.sqdist(v1,v2))\n\t}\n\n\tval clusteredData = parsedData.map(v => (clusters.predict(v),v)).cache() //Key: index of the appropriate cluster, Value: normalized point\n\t\t\t\t\t\n\tval indexedCenters = sc.parallelize(clusters.clusterCenters.map(cntr => (clusters.predict(cntr),cntr))).cache() //Key: index, Value: cluster center\n\n\t//Find the mean Silhouette score and the outliers for every cluster\n\tfor (c <- 0 to k-1)\n\t{\n\t\t//The points from current cluster (c)\n\t\tval currData = clusteredData.filter(r => r._1 == c)\n\t\t\t.map(r => r._2).cache() //They are all from the same cluster so we throw the cluster index\n\t\tval indexedCurrData = currData.zipWithIndex().cache() //We index them in order to give an id to every point\n\t\tval numPoints = currData.count() //Number of points in this cluster\n\n\t\t//Cartesian product in order to compute each point's distance from all other points in the same cluster\n\t\tval intraClusterProduct = indexedCurrData.cartesian(indexedCurrData)\n\t\t\t.map{case (x,y) => (x._2,vecDist(x._1,y._1))} //Key: the first point's id (from when it was indexed), Value: its distance from the second point. \n\t\t//In the cartesian product we also have points paired with themselves, but they don't affect our computations because their distance is 0 (doesn't affect the sum) and we divide by numPoints-1 (so that it doesn't affect the average)\n\t\t//ai = object i's average distance from the other objects in its cluster\n\t\tval ai = intraClusterProduct.reduceByKey((a,b) => a + b ) //Add the distances for elements with the same id (same point)\n\t\t\t.map(x =>(x._1, x._2/(numPoints-1))) //Key: point id, Value: average distance from its cluster\n    \t\t\t\n\t\t//All centers except the current one in order to find the closest to every point\n\t\tval otherCenters = indexedCenters.filter(x => x._1!=c )\n\t\tval nearestCenters = indexedCurrData.cartesian(otherCenters) //Every point from this cluster with every other center\n\t\t\t.map(r => (r._1._2,(r._2._1,r._1._1,vecDist(r._1._1,r._2._2)))) //Key: point id, Value: tuple (center index, point vector) \n\t\t\t.reduceByKey((a,b) => if (a._3 < b._3) a else b) //Min distance for every id \n\t\t\t.map(r => (r._2._1,(r._1, r._2._2))) //Key: index to center with min distance, Value: tuple(point id, point vector)\n\t\t//bi = object i's minimum average distance from objects of another cluster\n\t\tval bi = nearestCenters.join(clusteredData) //Join each point of this cluster with every point in the cluster with the nearest center (A workaround for computing bi granted that the data follow normal distribution)\n\t\t\t.map(r => (r._2._1._1,(vecDist(r._2._1._2,r._2._2),1))) //Key: first point's id, Value: tuple(distance of the two points,1)\n\t\t\t.reduceByKey((a,b) =>( a._1 + b._1, a._2 + b._2)) //Add the distances for every id (point) and count them \n\t\t\t.map(r => (r._1, r._2._1/r._2._2)) //Key: point id, Value: average distance from nearest cluster\n\n\t\tdef maxD(d1 : Double, d2: Double): Double = {\n\t\t\tif (d1>d2) return d1 else return d2\n\t\t}\n\t\t\n\t\tval s = ai.join(bi) //Join on point id\n\t\t\t.map(r => ((r._2._2 - r._2._1) / maxD(r._2._2,r._2._1), 1) ) //Key: Silhouette score for every point, Value: 1\n\t\t\t.reduce((a,b) => (a._1 + b._1, a._2 + b._2)) //Sum of silhouette scores, number of points\n\t\tval meanS = s._1 / s._2 //Average silhouette score for this cluster\t\t\n\n    println(\"Cluster \"+ (c+1)) \n\t\tprintln(\"Mean Silhouette Score: \"+ meanS)\t\n\t\tprintln()\t\n  }\n\n// Save and load model\nclusters.save(sc, outputDir)\n// Export to PMML to a String in PMML format\n//println(s\"PMML Model:\\n ${clusters.toPMML}\")\n\n// Export the model to a local file in PMML format\n//clusters.toPMML(outputDir+\"//kmeans.xml\")\n\n// Export the model to a directory on a distributed file system in PMML format\n//clusters.toPMML(sc, outputDir+\"//kmeans\")\n\n// Export the model to the OutputStream in PMML format\nclusters.toPMML(System.out)\n\nsc.stop() \n}\n}"}})[0m[0J
[0m[[0m[0mdebug[0m] [0m[0mUnhandled request received: shutdown: JsonRpcRequestMessage(2.0, ♨1, shutdown, null})[0m[0J
