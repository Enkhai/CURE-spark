[0m[[0m[0mdebug[0m] [0m[0m> Exec(;Test/compile; collectAnalyses, None, Some(CommandSource(network-1)))[0m[0J
[0m[[0m[0mdebug[0m] [0m[0mjsonRpcNotify: JsonRpcNotificationMessage(2.0, build/logMessage, {"type":4,"message":"Processing"})[0m[0J
[0m[[0m[0mdebug[0m] [0m[0mjsonRpcNotify: JsonRpcNotificationMessage(2.0, build/logMessage, {"type":4,"message":"Done"})[0m[0J
[0m[[0m[0mdebug[0m] [0m[0m> Exec(Test/compile, None, Some(CommandSource(network-1)))[0m[0J
[0m[[0m[0mdebug[0m] [0m[0mjsonRpcNotify: JsonRpcNotificationMessage(2.0, build/logMessage, {"type":4,"message":"Processing"})[0m[0J
[0m[[0m[0mdebug[0m] [0m[0mEvaluating tasks: Test / compile[0m[0J
[0m[[0m[0mdebug[0m] [0m[0mRunning task... Cancel: bloop.integrations.sbt.Offloader$$anon$1@786ff0ea, check cycles: false, forcegc: true[0m[0J
[0m[[0m[0mdebug[0m] [0m[0mjsonRpcNotify: JsonRpcNotificationMessage(2.0, build/taskStart, {"taskId":{"id":"5","parents":[]},"eventTime":1621993507285,"message":"Compiling will-karas","dataKind":"compile-task","data":{"target":{"uri":"file:/C:/Users/gmixo/Desktop/Will-Karas/#will-karas/Compile"}}})[0m[0J
[0m[[0m[0minfo[0m] [0m[0mcompiling 1 Scala source to C:\Users\gmixo\Desktop\Will-Karas\target\scala-2.12\classes ...[0m[0J
[0m[[0m[0mdebug[0m] [0m[0mjsonRpcNotify: JsonRpcNotificationMessage(2.0, build/logMessage, {"type":3,"message":"compiling 1 Scala source to C:\\Users\\gmixo\\Desktop\\Will-Karas\\target\\scala-2.12\\classes ..."})[0m[0J
[0m[[0m[0mdebug[0m] [0m[0mjsonRpcNotify: JsonRpcNotificationMessage(2.0, build/publishDiagnostics, {"textDocument":{"uri":"file:///C:/Users/gmixo/Desktop/Will-Karas/src/main/scala/bi-kmeans.scala"},"buildTarget":{"uri":"file:/C:/Users/gmixo/Desktop/Will-Karas/#will-karas/Compile"},"diagnostics":[],"reset":true})[0m[0J
[0m[[0m[0mdebug[0m] [0m[0mjsonRpcNotify: JsonRpcNotificationMessage(2.0, build/publishDiagnostics, {"textDocument":{"uri":"file:///C:/Users/gmixo/Desktop/Will-Karas/src/main/scala/Kmeans.scala"},"buildTarget":{"uri":"file:/C:/Users/gmixo/Desktop/Will-Karas/#will-karas/Compile"},"diagnostics":[],"reset":true})[0m[0J
[0m[[0m[0mdebug[0m] [0m[0mjsonRpcNotify: JsonRpcNotificationMessage(2.0, build/taskFinish, {"taskId":{"id":"5","parents":[]},"eventTime":1621993508390,"message":"Compiled will-karas","status":1,"dataKind":"compile-report","data":{"target":{"uri":"file:/C:/Users/gmixo/Desktop/Will-Karas/#will-karas/Compile"},"errors":0,"warnings":0,"time":1105}})[0m[0J
[0m[[0m[0mdebug[0m] [0m[0mjsonRpcNotify: JsonRpcNotificationMessage(2.0, build/taskStart, {"taskId":{"id":"6","parents":[]},"eventTime":1621993508406,"message":"Compiling will-karas-test","dataKind":"compile-task","data":{"target":{"uri":"file:/C:/Users/gmixo/Desktop/Will-Karas/#will-karas/Test"}}})[0m[0J
[0m[[0m[0mdebug[0m] [0m[0mjsonRpcNotify: JsonRpcNotificationMessage(2.0, build/taskFinish, {"taskId":{"id":"6","parents":[]},"eventTime":1621993508424,"message":"Compiled will-karas-test","status":1,"dataKind":"compile-report","data":{"target":{"uri":"file:/C:/Users/gmixo/Desktop/Will-Karas/#will-karas/Test"},"errors":0,"warnings":0,"time":18}})[0m[0J
[0m[[0m[32msuccess[0m] [0m[0mTotal time: 1 s, completed 26 ÎœÎ±ÏŠ 2021 4:45:08 Ï€Î¼[0m[0J
[0m[[0m[0mdebug[0m] [0m[0mjsonRpcNotify: JsonRpcNotificationMessage(2.0, build/logMessage, {"type":4,"message":"Done"})[0m[0J
[0m[[0m[0mdebug[0m] [0m[0m> Exec(collectAnalyses, None, Some(CommandSource(network-1)))[0m[0J
[0m[[0m[0mdebug[0m] [0m[0mjsonRpcNotify: JsonRpcNotificationMessage(2.0, build/logMessage, {"type":4,"message":"Processing"})[0m[0J
[0m[[0m[0mdebug[0m] [0m[0mEvaluating tasks: Compile / collectAnalyses[0m[0J
[0m[[0m[0mdebug[0m] [0m[0mRunning task... Cancel: bloop.integrations.sbt.Offloader$$anon$1@31f7eb8, check cycles: false, forcegc: true[0m[0J
[0m[[0m[0mdebug[0m] [0m[0manalysis location (C:\Users\gmixo\Desktop\Will-Karas\target\scala-2.12\zinc\inc_compile_2.12.zip,true)[0m[0J
[0m[[0m[32msuccess[0m] [0m[0mTotal time: 0 s, completed 26 ÎœÎ±ÏŠ 2021 4:45:08 Ï€Î¼[0m[0J
[0m[[0m[0mdebug[0m] [0m[0mjsonRpcNotify: JsonRpcNotificationMessage(2.0, build/logMessage, {"type":4,"message":"Done"})[0m[0J
[0m[[0m[0mdebug[0m] [0m[0m> Exec(shell, None, None)[0m[0J
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: textDocument/didOpen: JsonRpcNotificationMessage(2.0, textDocument/didOpen, {"textDocument":{"uri":"file:///c%3A/Users/gmixo/Desktop/BigData2/CURE-spark/src/main/scala/Cure.scala","languageId":"scala","version":1,"text":"import clustering.cure.{CureAlgorithm, CureArgs}\r\nimport org.apache.hadoop.conf.Configuration\r\nimport org.apache.hadoop.fs.{FileSystem, Path}\r\nimport org.apache.log4j._\r\nimport org.apache.spark.{SparkConf, SparkContext}\r\n\r\nimport java.io.PrintWriter\r\nimport java.util.Date\r\n\r\nobject Cure {\r\n\r\n  def main(args: Array[String]): Unit = {\r\n    Logger.getLogger(\"org.apache.spark.SparkContext\").setLevel(Level.WARN)\r\n\r\n    val sparkConf = new SparkConf()\r\n      .setMaster(\"local[4]\")\r\n      .setAppName(\"Cure\")\r\n\r\n    val sc = new SparkContext(sparkConf)\r\n\r\n    val currentDir = System.getProperty(\"user.dir\")\r\n    val inputDir = \"file://\" + currentDir + \"/datasets/data_size2/data1.txt\"\r\n    val outputDir = \"file://\" + currentDir + \"/cureOutput\"\r\n\r\n    val cureArgs = CureArgs(10, 10, 0.3, 4, inputDir, 0.4, removeOutliers = true)\r\n\r\n    val startTime = System.currentTimeMillis()\r\n    val result = CureAlgorithm.start(cureArgs, sc).cache()\r\n    val endTime = System.currentTimeMillis()\r\n\r\n    val text = s\"Total time taken to assign clusters is : ${((endTime - startTime) * 1.0) / 1000} seconds\"\r\n    println(text)\r\n\r\n    val resultFile = outputDir + \"_\" + new Date().getTime.toString\r\n    result.map(x =>\r\n      x._1\r\n        .mkString(\",\")\r\n        .concat(s\",${x._2}\")\r\n    ).saveAsTextFile(resultFile)\r\n\r\n    val conf = new Configuration()\r\n    val fs = FileSystem.get(conf)\r\n    val output = fs.create(new Path(s\"$resultFile/runtime.txt\"))\r\n    val writer = new PrintWriter(output)\r\n    try\r\n      writer.write(text)\r\n    finally\r\n      writer.close()\r\n\r\n    sc.stop()\r\n  }\r\n}\r\n"}})[0m[0J
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: textDocument/didOpen: JsonRpcNotificationMessage(2.0, textDocument/didOpen, {"textDocument":{"uri":"file:///c%3A/Users/gmixo/Desktop/BigData2/CURE-spark/src/main/scala/clustering/cure/CureAlgorithm.scala","languageId":"scala","version":1,"text":"package clustering.cure\r\n\r\nimport clustering.structures.{Cluster, KDNode, KDPoint, KDTree, MinHeap}\r\nimport org.apache.spark.SparkContext\r\nimport org.apache.spark.broadcast.Broadcast\r\nimport org.apache.spark.rdd.RDD\r\n\r\nobject CureAlgorithm {\r\n\r\n  def start(cureArgs: CureArgs, sparkContext: SparkContext): RDD[(Array[Double], Int)] = {\r\n\r\n    val distFile = sparkContext.textFile(cureArgs.inputFile)\r\n      .map(_\r\n        .split(\",\")\r\n        .map(_.toDouble)\r\n      )\r\n\r\n    val sample = distFile\r\n      .sample(withReplacement = false, fraction = cureArgs.samplingRatio)\r\n      .repartition(cureArgs.partitions)\r\n    println(s\"The total size is ${distFile.count()} and sampled count is ${sample.count()}\")\r\n\r\n    val points = sample.map(a => {\r\n      val p = KDPoint(a)\r\n      p.cluster = Cluster(Array(p), Array(p), null, p)\r\n      p\r\n    }).cache()\r\n\r\n    val broadcastVariables = (sparkContext.broadcast(cureArgs.numClusters),\r\n      sparkContext.broadcast(cureArgs.numRepresentatives),\r\n      sparkContext.broadcast(cureArgs.shrinkingFactor),\r\n      sparkContext.broadcast(cureArgs.removeOutliers))\r\n\r\n    val clusters = points.mapPartitions(partition => cluster(partition, broadcastVariables))\r\n      .collect()\r\n    println(s\"Partitioned Execution finished Successfully. Collected all ${clusters.length} clusters at driver.\")\r\n\r\n    val reducedPoints = clusters.flatMap(_.representatives).toList\r\n    val kdTree = createKDTree(reducedPoints)\r\n    val cHeap = createHeapFromClusters(clusters.toList, kdTree)\r\n\r\n    var clustersShortOfMReps =\r\n      if (cureArgs.removeOutliers)\r\n        clusters.count(_.representatives.length < cureArgs.numRepresentatives)\r\n      else\r\n        0\r\n\r\n    // trim all clusters having less than the desired number of representatives\r\n    while (cHeap.heapSize - clustersShortOfMReps > cureArgs.numClusters) {\r\n      val c1 = cHeap.takeHead()\r\n      val nearest = c1.nearest\r\n      val c2 = merge(c1, nearest, cureArgs.numRepresentatives, cureArgs.shrinkingFactor)\r\n\r\n      if (cureArgs.removeOutliers) {\r\n        val a = nearest.representatives.length < cureArgs.numRepresentatives\r\n        val b = c1.representatives.length < cureArgs.numRepresentatives\r\n        val c = c2.representatives.length < cureArgs.numRepresentatives\r\n\r\n        if (a && b && c) clustersShortOfMReps = clustersShortOfMReps - 1\r\n        else if (a && b) clustersShortOfMReps = clustersShortOfMReps - 2\r\n        else if (a || b) clustersShortOfMReps = clustersShortOfMReps - 1\r\n      }\r\n\r\n      c1.representatives.foreach(kdTree.delete)\r\n      nearest.representatives.foreach(kdTree.delete)\r\n\r\n      val (newNearestCluster, nearestDistance) = getNearestCluster(c2, kdTree)\r\n      c2.nearest = newNearestCluster\r\n      c2.squaredDistance = nearestDistance\r\n\r\n      c2.representatives.foreach(kdTree.insert)\r\n      removeClustersFromHeapUsingReps(kdTree, cHeap, c1, nearest)\r\n      cHeap.insert(c2)\r\n      println(s\"Processing and merging clusters. Heap size is ${cHeap.heapSize}\")\r\n    }\r\n    println(s\"Merged clusters at driver.\\n\" +\r\n      s\"  Total clusters ${cHeap.heapSize}\\n\" +\r\n      s\"  Removed $clustersShortOfMReps clusters without ${cureArgs.numRepresentatives} representatives\")\r\n\r\n    val finalClusters = cHeap.getDataArray\r\n      .slice(0, cHeap.heapSize)\r\n      .filter(_.representatives.length >= cureArgs.numRepresentatives)\r\n    finalClusters.zipWithIndex\r\n      .foreach { case (x, i) => x.id = i }\r\n\r\n    println(\"Final Representatives\")\r\n    finalClusters\r\n      .foreach(c =>\r\n        c.representatives\r\n          .foreach(r => println(s\"$r , ${c.id}\"))\r\n      )\r\n\r\n    val broadcastTree = sparkContext.broadcast(kdTree)\r\n    distFile.mapPartitions(partition => {\r\n      partition.map(p => {\r\n        (p, broadcastTree.value\r\n          .closestPointOfOtherCluster(KDPoint(p))\r\n          .cluster\r\n          .id)\r\n      })\r\n    })\r\n  }\r\n\r\n  private def cluster(partition: Iterator[KDPoint],\r\n                      broadcasts: (Broadcast[Int],\r\n                        Broadcast[Int],\r\n                        Broadcast[Double],\r\n                        Broadcast[Boolean])): Iterator[Cluster] = {\r\n\r\n    val (numClusters, numRepresentatives, shrinkingFactor, removeOutliers) = broadcasts\r\n\r\n    val partitionList = partition.toList\r\n\r\n    if (partitionList.length <= numClusters.value)\r\n      return partitionList\r\n        .map(p => Cluster(Array(p), Array(p), null, p))\r\n        .toIterator\r\n\r\n    val kdTree = createKDTree(partitionList)\r\n    val cHeap = createHeap(partitionList, kdTree)\r\n\r\n    if (removeOutliers.value) {\r\n      computeClustersAtPartitions(numClusters.value * 2, numRepresentatives.value, shrinkingFactor.value, kdTree, cHeap)\r\n      for (i <- 0 until cHeap.heapSize)\r\n        if (cHeap.getDataArray(i).representatives.length < numRepresentatives.value)\r\n          cHeap.remove(i)\r\n    }\r\n    computeClustersAtPartitions(numClusters.value, numRepresentatives.value, shrinkingFactor.value, kdTree, cHeap)\r\n\r\n    cHeap.getDataArray\r\n      .slice(0, cHeap.heapSize)\r\n      .map(c => {\r\n        c.points.foreach(_.cluster = null)\r\n        val newCluster = Cluster(findMFarthestPoints(c.points, c.mean, numRepresentatives.value),\r\n          c.representatives,\r\n          null,\r\n          c.mean,\r\n          c.squaredDistance)\r\n        newCluster.representatives.foreach(_.cluster = newCluster)\r\n        newCluster\r\n      }).toIterator\r\n  }\r\n\r\n  private def createKDTree(data: List[KDPoint]): KDTree = {\r\n    val kdTree = KDTree(KDNode(data.head, null, null), data.head.dimensions.length)\r\n    for (i <- 1 until data.length)\r\n      kdTree.insert(data(i))\r\n    kdTree\r\n  }\r\n\r\n  private def createHeap(data: List[KDPoint], kdTree: KDTree) = {\r\n    val cHeap = MinHeap(data.length)\r\n    data.map(p => {\r\n      val closest = kdTree.closestPointOfOtherCluster(p)\r\n      p.cluster.nearest = closest.cluster\r\n      p.cluster.squaredDistance = p.squaredDistance(closest)\r\n      cHeap.insert(p.cluster)\r\n      p.cluster\r\n    })\r\n    cHeap\r\n  }\r\n\r\n  private def createHeapFromClusters(data: List[Cluster], kdTree: KDTree): MinHeap = {\r\n    val cHeap = MinHeap(data.length)\r\n    data.foreach(p => {\r\n      val (closest, distance) = getNearestCluster(p, kdTree)\r\n      p.nearest = closest\r\n      p.squaredDistance = distance\r\n      cHeap.insert(p)\r\n    })\r\n    cHeap\r\n  }\r\n\r\n  private def computeClustersAtPartitions(numClusters: Int,\r\n                                          numRepresentatives: Int,\r\n                                          sf: Double,\r\n                                          kdTree: KDTree,\r\n                                          cHeap: MinHeap): Unit = {\r\n    while (cHeap.heapSize > numClusters) {\r\n      val c1 = cHeap.takeHead()\r\n      val nearest = c1.nearest\r\n      val c2 = merge(c1, nearest, numRepresentatives, sf)\r\n\r\n      c1.representatives.foreach(kdTree.delete)\r\n      nearest.representatives.foreach(kdTree.delete)\r\n\r\n      val (newNearestCluster, nearestDistance) = getNearestCluster(c2, kdTree)\r\n      c2.nearest = newNearestCluster\r\n      c2.squaredDistance = nearestDistance\r\n      c2.representatives.foreach(kdTree.insert)\r\n\r\n      removeClustersFromHeapUsingReps(kdTree, cHeap, c1, nearest)\r\n\r\n      cHeap.insert(c2)\r\n    }\r\n  }\r\n\r\n  private def removeClustersFromHeapUsingReps(kdTree: KDTree, cHeap: MinHeap, cluster: Cluster, nearest: Cluster): Unit = {\r\n    val heapSize = cHeap.heapSize\r\n    var i = 0\r\n    while (i < heapSize) {\r\n      var continue = true\r\n      val currCluster = cHeap.getDataArray(i)\r\n      val currNearest = currCluster.nearest\r\n      if (currCluster == nearest) {\r\n        cHeap.remove(i)\r\n        continue = false\r\n      }\r\n      if (currNearest == nearest || currNearest == cluster) {\r\n        val (newCluster, newDistance) = getNearestCluster(currCluster, kdTree)\r\n        currCluster.nearest = newCluster\r\n        currCluster.squaredDistance = newDistance\r\n        cHeap.heapify(i)\r\n        continue = false\r\n      }\r\n      if (continue) i += 1\r\n    }\r\n  }\r\n\r\n  private def getNearestCluster(cluster: Cluster, kdTree: KDTree): (Cluster, Double) = {\r\n    val (nearestRep, nearestDistance) = cluster\r\n      .representatives\r\n      .foldLeft(null: KDPoint, Double.MaxValue) {\r\n        case ((currNearestRep, currNearestDistance), rep) =>\r\n          val nearestRep = kdTree.closestPointOfOtherCluster(rep)\r\n          val nearestDistance = rep.squaredDistance(nearestRep)\r\n          if (nearestDistance < currNearestDistance)\r\n            (nearestRep, nearestDistance)\r\n          else\r\n            (currNearestRep, currNearestDistance)\r\n      }\r\n    (nearestRep.cluster, nearestDistance)\r\n  }\r\n\r\n  def copyPointsArray(oldArray: Array[KDPoint]): Array[KDPoint] = {\r\n    oldArray\r\n      .clone()\r\n      .map(p => {\r\n        if (p == null)\r\n          return null\r\n        KDPoint(p.dimensions.clone())\r\n      })\r\n  }\r\n\r\n  private def merge(cluster: Cluster, nearest: Cluster, repCount: Int, sf: Double): Cluster = {\r\n    val mergedPoints = cluster.points ++ nearest.points\r\n    val mean = meanOfPoints(mergedPoints)\r\n    var representatives = mergedPoints\r\n    if (mergedPoints.length > repCount)\r\n      representatives = findMFarthestPoints(mergedPoints, mean, repCount)\r\n    representatives = shrinkRepresentativeArray(sf, representatives, mean)\r\n\r\n    val mergedCl = Cluster(mergedPoints, representatives, null, mean)\r\n\r\n    mergedCl.representatives.foreach(_.cluster = mergedCl)\r\n    mergedCl.points.foreach(_.cluster = mergedCl)\r\n    mergedCl.mean.cluster = mergedCl\r\n\r\n    mergedCl\r\n  }\r\n\r\n  private def findMFarthestPoints(points: Array[KDPoint], mean: KDPoint, m: Int): Array[KDPoint] = {\r\n    val tmpArray = new Array[KDPoint](m)\r\n    for (i <- 0 until m) {\r\n      var maxDist = 0.0d\r\n      var minDist = 0.0d\r\n      var maxPoint: KDPoint = null\r\n\r\n      points.foreach(p => {\r\n        if (!tmpArray.contains(p)) {\r\n          if (i == 0) minDist = p.squaredDistance(mean)\r\n          else {\r\n            minDist = tmpArray.foldLeft(Double.MaxValue) { (maxd, r) => {\r\n              if (r == null) maxd\r\n              else {\r\n                val dist = p.squaredDistance(r)\r\n                if (dist < maxd) dist\r\n                else maxd\r\n              }\r\n            }\r\n            }\r\n          }\r\n          if (minDist >= maxDist) {\r\n            maxDist = minDist\r\n            maxPoint = p\r\n          }\r\n        }\r\n      })\r\n      tmpArray(i) = maxPoint\r\n    }\r\n    tmpArray.filter(_ != null)\r\n  }\r\n\r\n  private def shrinkRepresentativeArray(sf: Double, repArray: Array[KDPoint], mean: KDPoint): Array[KDPoint] = {\r\n    val tmpArray = copyPointsArray(repArray)\r\n    tmpArray.foreach(rep => {\r\n      val repDim = rep.dimensions\r\n      repDim.indices\r\n        .foreach(i => repDim(i) += (mean.dimensions(i) - repDim(i)) * sf)\r\n    })\r\n    tmpArray\r\n  }\r\n\r\n  def meanOfPoints(points: Array[KDPoint]): KDPoint = {\r\n    KDPoint(points\r\n      .filter(_ != null)\r\n      .map(_.dimensions)\r\n      .transpose\r\n      .map(x => {\r\n        x.sum / x.length\r\n      }))\r\n  }\r\n}\r\n"}})[0m[0J
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: textDocument/didClose: JsonRpcNotificationMessage(2.0, textDocument/didClose, {"textDocument":{"uri":"file:///c%3A/Users/gmixo/Desktop/BigData2/CURE-spark/src/main/scala/Cure.scala"}})[0m[0J
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: textDocument/didClose: JsonRpcNotificationMessage(2.0, textDocument/didClose, {"textDocument":{"uri":"file:///c%3A/Users/gmixo/Desktop/BigData2/CURE-spark/src/main/scala/clustering/cure/CureAlgorithm.scala"}})[0m[0J
[0m[[0m[0mdebug[0m] [0m[0mUnhandled request received: shutdown: JsonRpcRequestMessage(2.0, â™¨1, shutdown, null})[0m[0J
