[0m[[0m[0mdebug[0m] [0m[0m> Exec(collectAnalyses, None, Some(CommandSource(network-1)))[0m[0J
[0m[[0m[0mdebug[0m] [0m[0mjsonRpcNotify: JsonRpcNotificationMessage(2.0, build/logMessage, {"type":4,"message":"Processing"})[0m[0J
[0m[[0m[0mdebug[0m] [0m[0mEvaluating tasks: Compile / collectAnalyses[0m[0J
[0m[[0m[0mdebug[0m] [0m[0mRunning task... Cancel: bloop.integrations.sbt.Offloader$$anon$1@78eafad, check cycles: false, forcegc: true[0m[0J
[0m[[0m[0mdebug[0m] [0m[0manalysis location (C:\Users\gmixo\Desktop\Will-Karas\target\scala-2.12\zinc\inc_compile_2.12.zip,true)[0m[0J
[0m[[0m[32msuccess[0m] [0m[0mTotal time: 0 s, completed 26 ÎœÎ±ÏŠ 2021 4:39:59 Ï€Î¼[0m[0J
[0m[[0m[0mdebug[0m] [0m[0mjsonRpcNotify: JsonRpcNotificationMessage(2.0, build/logMessage, {"type":4,"message":"Done"})[0m[0J
[0m[[0m[0mdebug[0m] [0m[0m> Exec(shell, None, None)[0m[0J
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: textDocument/didOpen: JsonRpcNotificationMessage(2.0, textDocument/didOpen, {"textDocument":{"uri":"file:///c%3A/Users/gmixo/Desktop/Will-Karas/src/main/scala/Kmeans.scala","languageId":"scala","version":1,"text":"import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}\nimport org.apache.spark.mllib.linalg.{Vectors, Vector}\nimport org.apache.spark.ml.evaluation.ClusteringEvaluator\n\nimport org.apache.log4j._\nimport org.apache.spark.{SparkConf, SparkContext}\n\nobject kmeans {\n\n  def main(args: Array[String]): Unit = {\n\n    Logger.getLogger(\"org.apache.spark.SparkContext\").setLevel(Level.WARN)\n\n    val sparkConf = new SparkConf()\n      .setMaster(\"local[*]\")\n      .setAppName(\"KMEANS\")\n\n    val sc = new SparkContext(sparkConf)\n\nval currentDir = System.getProperty(\"user.dir\")\nval data = \"file:///\" + currentDir + \"\\\\data1.txt\"\nval outputDir = \"file:///\" + currentDir + \"\\\\output\"\n\nval parsedData = sc.textFile(data).map(s => Vectors.dense(s.split(',').map(_.toDouble))).cache()\n\n// Cluster the data into two classes using KMeans\nval numClusters = 5\n\nval numIterations = 20\nval initializationmode= \"KMeans.K_MEANS_PARALLEL\"\nval clusters = KMeans.train(parsedData, numClusters, numIterations, initializationmode)\nval k = numClusters \n// Evaluate clustering by computing Within Set Sum of Squared Errors\nval WSSSE = clusters.computeCost(parsedData)\nprintln(s\"Within Set Sum of Squared Errors = $WSSSE\")\n\n\n//Returns the Euclidian distance between vectors v1,v2\n\tdef vecDist( v1:Vector, v2:Vector ) : Double = {\n\t  return Math.sqrt(Vectors.sqdist(v1,v2))\n\t}\n\n\tval clusteredData = parsedData.map(v => (clusters.predict(v),v)).cache() //Key: index of the appropriate cluster, Value: normalized point\n\t\t\t\t\t\n\tval indexedCenters = sc.parallelize(clusters.clusterCenters.map(cntr => (clusters.predict(cntr),cntr))).cache() //Key: index, Value: cluster center\n\n\t//Find the mean Silhouette score and the outliers for every cluster\n\tfor (c <- 0 to k-1)\n\t{\n\t\t//The points from current cluster (c)\n\t\tval currData = clusteredData.filter(r => r._1 == c)\n\t\t\t.map(r => r._2).cache() //They are all from the same cluster so we throw the cluster index\n\t\tval indexedCurrData = currData.zipWithIndex().cache() //We index them in order to give an id to every point\n\t\tval numPoints = currData.count() //Number of points in this cluster\n\n\t\t//Cartesian product in order to compute each point's distance from all other points in the same cluster\n\t\tval intraClusterProduct = indexedCurrData.cartesian(indexedCurrData)\n\t\t\t.map{case (x,y) => (x._2,vecDist(x._1,y._1))} //Key: the first point's id (from when it was indexed), Value: its distance from the second point. \n\t\t//In the cartesian product we also have points paired with themselves, but they don't affect our computations because their distance is 0 (doesn't affect the sum) and we divide by numPoints-1 (so that it doesn't affect the average)\n\t\t//ai = object i's average distance from the other objects in its cluster\n\t\tval ai = intraClusterProduct.reduceByKey((a,b) => a + b ) //Add the distances for elements with the same id (same point)\n\t\t\t.map(x =>(x._1, x._2/(numPoints-1))) //Key: point id, Value: average distance from its cluster\n    \t\t\t\n\t\t//All centers except the current one in order to find the closest to every point\n\t\tval otherCenters = indexedCenters.filter(x => x._1!=c )\n\t\tval nearestCenters = indexedCurrData.cartesian(otherCenters) //Every point from this cluster with every other center\n\t\t\t.map(r => (r._1._2,(r._2._1,r._1._1,vecDist(r._1._1,r._2._2)))) //Key: point id, Value: tuple (center index, point vector) \n\t\t\t.reduceByKey((a,b) => if (a._3 < b._3) a else b) //Min distance for every id \n\t\t\t.map(r => (r._2._1,(r._1, r._2._2))) //Key: index to center with min distance, Value: tuple(point id, point vector)\n\t\t//bi = object i's minimum average distance from objects of another cluster\n\t\tval bi = nearestCenters.join(clusteredData) //Join each point of this cluster with every point in the cluster with the nearest center (A workaround for computing bi granted that the data follow normal distribution)\n\t\t\t.map(r => (r._2._1._1,(vecDist(r._2._1._2,r._2._2),1))) //Key: first point's id, Value: tuple(distance of the two points,1)\n\t\t\t.reduceByKey((a,b) =>( a._1 + b._1, a._2 + b._2)) //Add the distances for every id (point) and count them \n\t\t\t.map(r => (r._1, r._2._1/r._2._2)) //Key: point id, Value: average distance from nearest cluster\n\n\t\tdef maxD(d1 : Double, d2: Double): Double = {\n\t\t\tif (d1>d2) return d1 else return d2\n\t\t}\n\t\t\n\t\tval s = ai.join(bi) //Join on point id\n\t\t\t.map(r => ((r._2._2 - r._2._1) / maxD(r._2._2,r._2._1), 1) ) //Key: Silhouette score for every point, Value: 1\n\t\t\t.reduce((a,b) => (a._1 + b._1, a._2 + b._2)) //Sum of silhouette scores, number of points\n\t\tval meanS = s._1 / s._2 //Average silhouette score for this cluster\t\t\n\n    println(\"Cluster \"+ (c+1)) \n\t\tprintln(\"Mean Silhouette Score: \"+ meanS)\t\n\t\tprintln()\t\n  }\n\n// Save and load model\nclusters.save(sc, outputDir)\n// Export to PMML to a String in PMML format\n//println(s\"PMML Model:\\n ${clusters.toPMML}\")\n\n// Export the model to a local file in PMML format\n//clusters.toPMML(outputDir+\"//kmeans.xml\")\n\n// Export the model to a directory on a distributed file system in PMML format\n//clusters.toPMML(sc, outputDir+\"//kmeans\")\n\n// Export the model to the OutputStream in PMML format\nclusters.toPMML(System.out)\n\nsc.stop() \n}\n}"}})[0m[0J
